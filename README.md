# crawler
Авторы: Новоселов Кирилл, Коптева Татьяна

# Usage
`crawler.py START_URL [OPTIONS]'

- `-d`, `--domains`: список разрешенных доменов
- `-f`, `--file`: путь к файлу, содержащему список разрешенных доменов
- `-m`, `--max_depth`: максимальная глубина загрузки
- `-z`, `--zip`: путь для сохранения результатов в zip-архиве
- `-o`, `--folder`: путь для сохранения результатов в папке
- `-t`, `--num_threads`: количество потоков, используемых для загрузки страниц
- `-i`, `--ignore_robots`: проигнорировать файл `robots.txt` и загрузить все страницы
